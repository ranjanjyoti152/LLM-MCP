<p align="center">
  <h1 align="center">üß† LLM Memory MCP Server</h1>
  <p align="center">
    <strong>A unified, persistent memory layer for all your AI coding assistants.</strong>
  </p>
  <p align="center">
    Save conversations, knowledge, and context from <em>any</em> platform ‚Äî retrieve it from <em>every</em> platform.
  </p>
  <p align="center">
    <a href="#-quick-start"><img src="https://img.shields.io/badge/Get_Started-blue?style=for-the-badge" alt="Get Started"></a>
    <img src="https://img.shields.io/badge/python-3.12-blue?style=flat-square&logo=python&logoColor=white" alt="Python 3.12">
    <img src="https://img.shields.io/badge/PostgreSQL-16-336791?style=flat-square&logo=postgresql&logoColor=white" alt="PostgreSQL 16">
    <img src="https://img.shields.io/badge/Docker-Compose-2496ED?style=flat-square&logo=docker&logoColor=white" alt="Docker">
    <img src="https://img.shields.io/badge/MCP-Streamable_HTTP-green?style=flat-square" alt="MCP">
    <img src="https://img.shields.io/badge/License-MIT-yellow?style=flat-square" alt="MIT">
  </p>
</p>

---

## üí° The Problem

You use **multiple AI assistants** daily ‚Äî Antigravity, Cursor, VS Code Copilot, Gemini, ChatGPT ‚Äî but **none of them remember** what you discussed in the other.  

Every time you switch platforms, you start from scratch. Your preferences, project context, past decisions ‚Äî all lost.

## ‚ú® The Solution

**LLM Memory MCP Server** gives every AI platform a **shared brain**. It stores your conversations, knowledge, and preferences in a PostgreSQL database, accessible via the open [Model Context Protocol (MCP)](https://modelcontextprotocol.io) standard.

> **Save a fact in Cursor ‚Üí recall it in Antigravity ‚Üí search for it in VS Code.**

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        YOUR AI PLATFORMS                             ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Antigravity  ‚îÇ ‚îÇ Cursor ‚îÇ ‚îÇ VS Code  ‚îÇ ‚îÇ Gemini ‚îÇ ‚îÇ  ChatGPT  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   (Google)   ‚îÇ ‚îÇ        ‚îÇ ‚îÇ Copilot  ‚îÇ ‚îÇ  CLI   ‚îÇ ‚îÇ  / Claude ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ         ‚îÇ             ‚îÇ           ‚îÇ            ‚îÇ            ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ             ‚îÇ           ‚îÇ            ‚îÇ            ‚îÇ
          ‚ñº             ‚ñº           ‚ñº            ‚ñº            ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ              Streamable HTTP  (:4040/mcp)                   ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
    ‚îÇ  ‚îÇ             üß† LLM Memory MCP Server                  ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ  ‚îÇ   7 MCP Tools    ‚îÇ  ‚îÇ    2 MCP Resources     ‚îÇ     ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ  ‚îÇ  save, search,   ‚îÇ  ‚îÇ  stats, platforms      ‚îÇ     ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ  ‚îÇ  retrieve, delete‚îÇ  ‚îÇ                        ‚îÇ     ‚îÇ  ‚îÇ
    ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ  ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ                        ‚îÇ
                   ‚ñº                        ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                 PostgreSQL 16  (:4569)                       ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
    ‚îÇ  ‚îÇ conversations   ‚îÇ ‚îÇ messages ‚îÇ ‚îÇ    knowledge      ‚îÇ      ‚îÇ
    ‚îÇ  ‚îÇ (platform, tags)‚îÇ ‚îÇ (role,   ‚îÇ ‚îÇ (category, tags,  ‚îÇ      ‚îÇ
    ‚îÇ  ‚îÇ                 ‚îÇ ‚îÇ  content)‚îÇ ‚îÇ  source_platform) ‚îÇ      ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
    ‚îÇ              Full-Text Search Indexes (GIN)                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ Supported Platforms

| Platform | Transport | Status |
|:---------|:----------|:------:|
| **Antigravity** (Google) | Streamable HTTP | ‚úÖ Ready |
| **Cursor** | Streamable HTTP | ‚úÖ Ready |
| **VS Code** + GitHub Copilot | Streamable HTTP | ‚úÖ Ready |
| **Gemini CLI** | Streamable HTTP | ‚úÖ Ready |
| **Claude Desktop** | Streamable HTTP | ‚úÖ Ready |
| **ChatGPT** (MCP-compatible) | Streamable HTTP | ‚úÖ Ready |
| Any MCP-compatible client | Streamable HTTP | ‚úÖ Ready |

---

## üöÄ Quick Start

### Prerequisites

- [Docker](https://docs.docker.com/get-docker/) & [Docker Compose](https://docs.docker.com/compose/install/) installed
- Git (optional, for cloning)

### 1. Clone & Start

```bash
git clone https://github.com/YOUR_USERNAME/LLM-MCP.git
cd LLM-MCP

# Start PostgreSQL + MCP Server
docker compose up -d
```

### 2. Verify

```bash
# Check containers are healthy
docker compose ps

# Expected output:
# llm-mcp-postgres   ... Up (healthy)   0.0.0.0:4569->5432/tcp
# llm-mcp-server     ... Up             0.0.0.0:4040->4040/tcp
```

### 3. Connect Your AI Platform

Add the MCP server URL to your platform's config (see [detailed guides below](#-platform-configuration)):

```
http://localhost:4040/mcp
```

### 4. Test It!

Ask your AI assistant:

> *"Save a knowledge entry: I prefer Python for backend and use Docker for everything."*

Switch to a **different** AI platform and ask:

> *"Search my memory for my tech preferences."*

‚ú® The knowledge persists across all your platforms!

---

## üîß Platform Configuration

### <img src="https://img.shields.io/badge/-Antigravity-4285F4?style=flat-square&logo=google&logoColor=white" alt="Antigravity"> Antigravity (Google)

**Option A** ‚Äî Via UI: Go to **Settings ‚Üí MCP Servers ‚Üí Add** and paste the URL.

**Option B** ‚Äî Via config file (`mcp_config.json`):

```json
{
  "mcpServers": {
    "llm-memory": {
      "serverUrl": "http://localhost:4040/mcp"
    }
  }
}
```

---

### <img src="https://img.shields.io/badge/-Cursor-000000?style=flat-square&logo=cursor&logoColor=white" alt="Cursor"> Cursor

**Option A** ‚Äî Via UI: **Settings ‚Üí MCP Servers ‚Üí Add New MCP Server**

**Option B** ‚Äî Project-level config (`.cursor/mcp.json`):

```json
{
  "mcpServers": {
    "llm-memory": {
      "url": "http://localhost:4040/mcp"
    }
  }
}
```

**Option C** ‚Äî Global config (`~/.cursor/mcp.json`) ‚Äî applies to all projects.

---

### <img src="https://img.shields.io/badge/-VS_Code-007ACC?style=flat-square&logo=visualstudiocode&logoColor=white" alt="VS Code"> VS Code + GitHub Copilot

**Option A** ‚Äî Via Command Palette: `Ctrl+Shift+P` ‚Üí `MCP: Add Server` ‚Üí HTTP ‚Üí enter `http://localhost:4040/mcp`

**Option B** ‚Äî Workspace config (`.vscode/mcp.json`):

```json
{
  "servers": {
    "llm-memory": {
      "type": "http",
      "url": "http://localhost:4040/mcp"
    }
  }
}
```

**Option C** ‚Äî User settings (global): Add the same config to your VS Code user `settings.json` under `"mcp"`.

---

### <img src="https://img.shields.io/badge/-Gemini_CLI-8E75B2?style=flat-square&logo=googlegemini&logoColor=white" alt="Gemini"> Gemini CLI

Edit `~/.gemini/settings.json`:

```json
{
  "mcpServers": {
    "llm-memory": {
      "httpUrl": "http://localhost:4040/mcp"
    }
  }
}
```

---

### <img src="https://img.shields.io/badge/-Claude-D4A574?style=flat-square" alt="Claude"> Claude Desktop

**Option A - Local (Best Performance)**
If you are running the server on the same machine as Claude, connect directly via Docker.
**This requires no extra tools and is the fastest method.**

Go to **Settings ‚Üí Developer ‚Üí Edit Config** (`claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "llm-memory": {
      "command": "docker",
      "args": [
        "exec",
        "-i",
        "llm-mcp-server",
        "python",
        "server.py",
        "stdio"
      ]
    }
  }
}
```

### <img src="https://img.shields.io/badge/-Claude-D4A574?style=flat-square" alt="Claude"> Claude Desktop

**Option A - Local (Best Performance)**
If you are running the server on the same machine as Claude, connect directly via Docker.
**This requires no extra tools and is the fastest method.**

Go to **Settings ‚Üí Developer ‚Üí Edit Config** (`claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "llm-memory": {
      "command": "docker",
      "args": [
        "exec",
        "-i",
        "llm-mcp-server",
        "python",
        "server.py",
        "stdio"
      ]
    }
  }
}
```

**Option B - Remote / HTTP (Universal)**
To connect to a remote server (e.g., `https://mcp.smartnvr.shop/mcp`), use `supergateway` as the bridge. This requires Node.js installed locally.

```json
{
  "mcpServers": {
    "llm-memory": {
      "command": "npx",
      "args": [
        "-y",
        "supergateway",
        "--sse",
        "https://mcp.smartnvr.shop/mcp"
      ]
    }
  }
}
```

*(Note: SSH Tunnel is also a reliable alternative if you prefer not to use Node.js)*

---

### <img src="https://img.shields.io/badge/-ChatGPT-74AA9C?style=flat-square&logo=openai&logoColor=white" alt="ChatGPT"> ChatGPT / Other MCP Clients

For any platform that supports MCP via HTTP, use:

```
Endpoint:   http://localhost:4040/mcp
Transport:  Streamable HTTP (POST/GET with optional SSE streaming)
```

---

## üõ†Ô∏è Available MCP Tools

| Tool | Description | Example Use |
|:-----|:------------|:------------|
| `save_conversation` | Save a full conversation with messages, platform tag, and metadata | *"Save this conversation about Docker debugging"* |
| `search_memory` | Full-text search across all stored conversations & messages | *"What did we discuss about authentication last week?"* |
| `get_recent_conversations` | Retrieve latest conversations, optionally filtered by platform | *"Show me my recent Cursor conversations"* |
| `save_knowledge` | Store a fact, preference, instruction, or decision | *"Remember: I use PostgreSQL 16 for all projects"* |
| `search_knowledge` | Search knowledge entries by query, category, or tags | *"What are my coding preferences?"* |
| `get_context_summary` | Get a combined overview of knowledge + conversations for a topic | *"Give me context on the payment system project"* |
| `delete_memory` | Delete a specific conversation or knowledge entry by ID | *"Delete conversation abc-123"* |

### üìä MCP Resources

| Resource URI | Description |
|:-------------|:------------|
| `memory://stats` | Database statistics ‚Äî total conversations, messages, knowledge items, platform breakdown |
| `memory://platforms` | List of all AI platforms that have stored data |

---

## üìÅ Project Structure

```
LLM-MCP/
‚îú‚îÄ‚îÄ docker-compose.yml       # Orchestrates PostgreSQL + MCP Server
‚îú‚îÄ‚îÄ Dockerfile               # Python 3.12 slim container for MCP server
‚îú‚îÄ‚îÄ .env                     # Environment variables (ports, credentials)
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ server.py                # FastMCP server ‚Äî 7 tools + 2 resources
‚îú‚îÄ‚îÄ db.py                    # Async database layer (asyncpg + full-text search)
‚îú‚îÄ‚îÄ test_client.py           # End-to-end verification script
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ README.md            # Per-platform configuration snippets
‚îî‚îÄ‚îÄ README.md                # This file
```

---

## ‚öôÔ∏è Configuration

All settings are in `.env`:

| Variable | Default | Description |
|:---------|:--------|:------------|
| `POSTGRES_PORT` | `4569` | Host port for PostgreSQL |
| `MCP_PORT` | `4040` | Host port for MCP server |
| `POSTGRES_USER` | `mcp_user` | Database username |
| `POSTGRES_PASSWORD` | `mcp_secure_pass_2026` | Database password |
| `POSTGRES_DB` | `mcp_memory` | Database name |
| `MCP_HOST` | `0.0.0.0` | MCP server bind address |

### Accessing from Other Machines (LAN)

If your AI platform runs on a different machine on your local network, replace `localhost` with your server's IP:

```
http://192.168.100.69:4040/mcp
```

---


## üß™ Testing

### Automated Test Client

```bash
# Install dependencies locally (if not using Docker)
pip install mcp[cli] asyncpg python-dotenv

# Run the full test suite
python test_client.py
```

The test client exercises all 7 tools and 2 resources, verifying:
- ‚úÖ Conversation save & retrieval
- ‚úÖ Knowledge storage & search
- ‚úÖ Full-text search across messages
- ‚úÖ Context summary generation
- ‚úÖ Resource endpoints (stats, platforms)
- ‚úÖ Delete operations

### Manual Verification

```bash
# Check PostgreSQL is accessible
docker exec llm-mcp-postgres psql -U mcp_user -d mcp_memory -c "SELECT COUNT(*) FROM conversations;"

# Check MCP server logs
docker logs -f llm-mcp-server

# Restart the stack
docker compose restart
```

---

## üìã Docker Commands Reference

| Command | Description |
|:--------|:------------|
| `docker compose up -d` | Start all services in background |
| `docker compose down` | Stop all services |
| `docker compose restart` | Restart all services |
| `docker compose logs -f mcp-server` | Stream MCP server logs |
| `docker compose ps` | Check service status |
| `docker compose down -v` | Stop & **delete all data** ‚ö†Ô∏è |

---

## üóÑÔ∏è Database Schema

Three core tables with full-text search indexes:

```sql
-- Stores conversation metadata (platform, title, summary, tags)
conversations  ‚Üí  one-to-many  ‚Üí  messages (role, content)

-- Stores standalone knowledge (facts, preferences, instructions)
knowledge (category, content, tags, source_platform)
```

**Indexes:** GIN indexes on `conversations`, `messages`, and `knowledge` for fast full-text search via PostgreSQL's `to_tsvector` / `plainto_tsquery`.

---

## üîí Security Notes

- By default, the server binds to `0.0.0.0` (accessible from your network)
- For **local-only** use, set `MCP_HOST=127.0.0.1` in `.env`
- Change the default `POSTGRES_PASSWORD` in production
- Consider adding a reverse proxy (nginx/Caddy) with TLS for remote access

---

## üó∫Ô∏è Roadmap

- [ ] Semantic search with `pgvector` embeddings
- [ ] Automatic conversation summarization
- [ ] Memory expiration / archival policies
- [ ] Web dashboard for browsing stored memories
- [ ] Authentication / API keys for multi-user support
- [ ] Webhook notifications on new memories

---

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/semantic-search`)
3. Commit your changes (`git commit -m 'Add semantic search with pgvector'`)
4. Push to the branch (`git push origin feature/semantic-search`)
5. Open a Pull Request

---

## üìÑ License

This project is licensed under the **MIT License** ‚Äî see the [LICENSE](LICENSE) file for details.

---

<p align="center">
  <strong>Built with ‚ù§Ô∏è for AI power users who use every tool available.</strong>
  <br>
  <em>Stop repeating yourself. Let your AIs share a brain.</em>
</p>
